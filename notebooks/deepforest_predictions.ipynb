{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree detection with DeepForest\n",
    "In this notebook, we reproduce the results of tree detection step of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, Generator\n",
    "import os\n",
    "from math import ceil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from deepforest import deepforest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define class for extracting patches from large RGB images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchGenerator:\n",
    "    def __init__(self, image: np.ndarray, patch_shape: Tuple[int, int]) -> None:\n",
    "        self.image = image\n",
    "        self.height, self.width, self.num_channels = self.image.shape\n",
    "        self.patch_shape = patch_shape\n",
    "        self._extract_patches()\n",
    "\n",
    "    def __call__(self) -> Generator[np.ndarray, None, None]:\n",
    "        for idx in range(self._calculate_num_patches()):\n",
    "            yield self.patches[idx, :, :, :]\n",
    "\n",
    "    def get_patch(self, idx: int) -> np.ndarray:\n",
    "        return self.patches[idx, :, :, :]\n",
    "\n",
    "    def get_patch_tl_br(self, idx: int) -> Tuple[int, int, int, int]:\n",
    "        i, j = self._calculate_patch_coord(idx)\n",
    "        v_start, v_end = i * self.patch_shape[0], (i + 1) * self.patch_shape[0]\n",
    "        u_start, u_end = j * self.patch_shape[1], (j + 1) * self.patch_shape[1]\n",
    "        return u_start, v_start, u_end, v_end\n",
    "\n",
    "    def get_num_patches(self) -> int:\n",
    "        return self._calculate_num_patches()\n",
    "\n",
    "    def stitch_patches(self, patches: np.ndarray = None) -> np.ndarray:\n",
    "        if patches is None:\n",
    "            patches = self.patches\n",
    "        stiched_image = np.zeros_like(self.image)\n",
    "        for idx in range(self._calculate_num_patches()):\n",
    "            u_start, v_start, u_end, v_end = self.get_patch_tl_br(idx)\n",
    "            patch_shape = stiched_image[v_start:v_end, u_start:u_end, :].shape\n",
    "            stiched_image[v_start:v_end, u_start:u_end, :] = patches[idx, :patch_shape[0], :patch_shape[1], :]\n",
    "        return stiched_image\n",
    "\n",
    "    def _extract_patches(self) -> np.ndarray:\n",
    "        num_patches = self._calculate_num_patches()\n",
    "        self.patches = np.zeros((num_patches, *self.patch_shape, self.num_channels))\n",
    "\n",
    "        for idx in range(num_patches):\n",
    "            u_start, v_start, u_end, v_end = self.get_patch_tl_br(idx)\n",
    "            patch = self.image[v_start:v_end, u_start:u_end, :]\n",
    "            patch_height, patch_width, _ = patch.shape\n",
    "            self.patches[idx, :patch_height, :patch_width, :] = patch\n",
    "\n",
    "    def _calculate_num_rows_columns(self) -> Tuple[int, int]:\n",
    "        num_rows = ceil(self.height / self.patch_shape[0])\n",
    "        num_columns = ceil(self.width / self.patch_shape[1])\n",
    "        return num_rows, num_columns\n",
    "\n",
    "    def _calculate_patch_coord(self, idx: int) -> Tuple[int, int]:\n",
    "        _, num_columns = self._calculate_num_rows_columns()\n",
    "        row_idx = idx // num_columns\n",
    "        col_idx = idx % num_columns\n",
    "        return row_idx, col_idx\n",
    "\n",
    "    def _calculate_num_patches(self) -> int:\n",
    "        num_rows, num_columns = self._calculate_num_rows_columns()\n",
    "        return num_rows * num_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define method for loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_filepath: str) -> np.ndarray:\n",
    "    return cv2.imread(image_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use deepforest model to predict bounding boxes of tree crowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILEPATH = \"../models/final_model_4000_epochs_35.h5\"\n",
    "PREDICTION_OUTPUT_DIR = \"../data/processed/predicted_bbox\"\n",
    "PATCH_SHAPE = (4000, 4000)\n",
    "SCORE_THRESHOLD = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0108 16:21:11.025869 140715621279552 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config file: /usr/local/lib/python3.6/dist-packages/deepforest/data/deepforest_config.yml\n",
      "Loading saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0108 16:21:13.271842 140715621279552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/deepforest/keras_retinanet/backend/tensorflow_backend.py:104: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0108 16:21:17.094801 140715621279552 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'Variable:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_1:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_2:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_3:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_4:0' shape=(9, 4) dtype=float32> anchors\n"
     ]
    }
   ],
   "source": [
    "model = deepforest.deepforest(saved_model=MODEL_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to predict boudning boxes for given orthomosaic (RGB image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bounding_boxes(orthomosaic_filepath):\n",
    "    orthomosaic = load_image(orthomosaic_filepath)\n",
    "    patch_generator = PatchGenerator(orthomosaic, PATCH_SHAPE)\n",
    "    prediction: pd.DataFrame = None\n",
    "    for idx, patch in enumerate(patch_generator()):\n",
    "        pred = model.predict_image(\n",
    "            numpy_image=patch, return_plot=False, score_threshold=SCORE_THRESHOLD\n",
    "        )\n",
    "        # Transform bounding box cooords from patch to orthomosaic\n",
    "        vmin, umin, _, _ = patch_generator.get_patch_tl_br(idx)\n",
    "        pred['xmin'] = pred['xmin'] + vmin\n",
    "        pred['xmax'] = pred['xmax'] + vmin\n",
    "        pred['ymin'] = pred['ymin'] + umin\n",
    "        pred['ymax'] = pred['ymax'] + umin\n",
    "        prediction = pd.concat([prediction, pred], ignore_index=True)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(prediction: pd.DataFrame, orthomosaic_filepath):\n",
    "    orthomosaic_filename = os.path.splitext(os.path.basename(orthomosaic_filepath))[0]\n",
    "    output_filename = os.path.join(PREDICTION_OUTPUT_DIR, f\"{orthomosaic_filename}_predicted_bounding_boxes.csv\")\n",
    "    prediction.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:17<01:29, 17.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          xmin         ymin         xmax         ymax     score label\n",
      "0  3472.602783  2084.359863  3740.785400  2330.677002  0.626113  Tree\n",
      "1  2925.480225  2230.653076  3442.990723  2764.071289  0.579985  Tree\n",
      "2  2368.995117  3021.250244  2828.616699  3481.844727  0.508287  Tree\n",
      "3  1431.577759  3564.384033  2030.847412  3988.493652  0.452210  Tree\n",
      "4  2025.169922  3700.907227  2357.979004  3998.964111  0.441826  Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:32<01:04, 16.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          xmin         ymin         xmax         ymax     score label\n",
      "0  2628.322021  3071.776367  2872.077393  3334.477783  0.602471  Tree\n",
      "1  1783.682373  3419.016113  2029.980103  3655.197998  0.581091  Tree\n",
      "2  2118.699463  3828.580811  2354.572021  3998.114502  0.558561  Tree\n",
      "3  3796.686279  2701.232666  3997.942871  3050.047607  0.538558  Tree\n",
      "4  2027.906372  2961.632324  2254.148438  3190.377197  0.523431  Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:52<00:53, 17.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          xmin         ymin         xmax         ymax     score label\n",
      "0  3232.731201  3694.286133  3471.770508  3949.400879  0.661207  Tree\n",
      "1  3406.867920  2998.459229  3763.383789  3375.258789  0.660219  Tree\n",
      "2  2095.669922  2800.014648  2545.101074  3340.188477  0.604788  Tree\n",
      "3  2893.671631  3154.630127  3156.359131  3415.236572  0.588699  Tree\n",
      "4  2558.736084  2591.417236  2845.240234  2848.051758  0.571798  Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [01:11<00:36, 18.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          xmin         ymin         xmax         ymax     score label\n",
      "0  3259.540283  3475.117676  3614.737549  3921.431641  0.432400  Tree\n",
      "1  2252.078613  3129.029785  3099.139648  3964.230713  0.409663  Tree\n",
      "2  3742.937988  3458.573242  3990.471191  3994.394531  0.333250  Tree\n",
      "3  1659.075317  3775.961914  2138.611816  3995.636963  0.320874  Tree\n",
      "4  3509.222412  3121.702637  3918.738037  3519.722656  0.286405  Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [01:23<00:16, 16.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          xmin         ymin         xmax         ymax     score label\n",
      "0  3142.790771  1472.712280  3974.216553  2416.628418  0.672816  Tree\n",
      "1  1724.132935  1950.795898  2367.895996  2568.429199  0.600508  Tree\n",
      "2    27.317541  1729.263306  1201.323730  2865.305664  0.552456  Tree\n",
      "3  2949.085693  2396.412842  3516.715576  2934.051270  0.518247  Tree\n",
      "4  2397.747070  3601.624756  2900.901367  3994.374756  0.507315  Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:39<00:00, 16.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          xmin         ymin         xmax         ymax     score label\n",
      "0  3632.374512  3633.669189  3994.532471  4000.000000  0.755147  Tree\n",
      "1  3137.673584  3205.398438  3501.337891  3626.599121  0.680561  Tree\n",
      "2  2953.614746  3629.650146  3325.656738  3995.960938  0.622315  Tree\n",
      "3  3611.481934  2435.647949  3986.134033  2857.687744  0.618454  Tree\n",
      "4  3353.430664  3779.258789  3622.084961  4000.000000  0.607611  Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "orthomosaic_filepaths = [\n",
    "    \"../data/raw/Carlos Vera Arteaga RGB.tif\",\n",
    "    \"../data/raw/Carlos Vera Guevara RGB.tif\",\n",
    "    \"../data/raw/Flora Pluas RGB.tif\",\n",
    "    \"../data/raw/Leonor Aspiazu RGB.tif\",\n",
    "    \"../data/raw/Manuel Macias RGB.tif\",\n",
    "    \"../data/raw/Nestor Macias RGB.tif\",\n",
    "]\n",
    "\n",
    "for orthomosaic_filepath in tqdm(orthomosaic_filepaths):\n",
    "    predictions = predict_bounding_boxes(orthomosaic_filepath)\n",
    "    print(predictions.head())\n",
    "    save_predictions(predictions, orthomosaic_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to generate visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORTHOMOSAICS_DIR = \"../data/raw/\"\n",
    "VISUALIZATION_OUTPUT_DIR = \"../data/processed/predicted_bbox_visualizations\"\n",
    "COLOR = (0, 0, 255)\n",
    "THICKNESS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bounding_boxes(orthomosaic_name):\n",
    "    # Load deepforest predictions\n",
    "    prediction_filename = f\"{orthomosaic_name}_predicted_bounding_boxes.csv\"\n",
    "    prediction_filepath = os.path.join(PREDICTION_OUTPUT_DIR, prediction_filename)\n",
    "    predictions = pd.read_csv(prediction_filepath)\n",
    "    # Load RGB orthomosaic\n",
    "    ortho_filename = f\"{orthomosaic_name}.tif\"\n",
    "    ortho_filepath = os.path.join(ORTHOMOSAICS_DIR, ortho_filename)\n",
    "    image = load_image(ortho_filepath)\n",
    "    # Draw bounding boxes\n",
    "    for _, row in predictions.iterrows():\n",
    "        p1 = (int(row.xmin), int(row.ymin))\n",
    "        p2 = (int(row.xmax), int(row.ymax))\n",
    "        image = cv2.rectangle(image, p1, p2, COLOR, THICKNESS)\n",
    "    # Save visualizations\n",
    "    output_filename = f\"{orthomosaic_name}_predicted_boxes.png\"\n",
    "    output_filepath = os.path.join(VISUALIZATION_OUTPUT_DIR, output_filename)\n",
    "    cv2.imwrite(output_filepath, image)\n",
    "    image_compressed = cv2.resize(image, None, fx=0.1, fy=0.1)\n",
    "    output_filename = f\"{VISUALIZATION_OUTPUT_DIR}_predicted_boxes_compressed.png\"\n",
    "    output_filepath = os.path.join(VISUALIZATION_OUTPUT_DIR, output_filename)\n",
    "    cv2.imwrite(output_filepath, image_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw and save visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:50<00:00,  8.48s/it]\n"
     ]
    }
   ],
   "source": [
    "orthomosaic_names = [\n",
    "    \"Carlos Vera Arteaga RGB\",\n",
    "    \"Carlos Vera Guevara RGB\",\n",
    "    \"Flora Pluas RGB\",\n",
    "    \"Leonor Aspiazu RGB\",\n",
    "    \"Manuel Macias RGB\",\n",
    "    \"Nestor Macias RGB\",\n",
    "]\n",
    "for ortho_name in tqdm(orthomosaic_names):\n",
    "    visualize_bounding_boxes(ortho_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
