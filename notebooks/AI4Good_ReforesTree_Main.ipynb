{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BpY1mA8Yz5Yz"
   },
   "source": [
    "ETHZ - 2022 \\\\\n",
    "\n",
    "AI4Good - ReforesTree project \\\\\n",
    "\n",
    "Klim Troyan & Silviu Nastasescu & Dominic Wong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0xOyjomsqCh"
   },
   "source": [
    "## **Requirements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-6jU_q5WYJy"
   },
   "outputs": [],
   "source": [
    "# The requirements.txt file should be in the working directory\n",
    "# The paths for the field dataset and images dataset (i.e, the tiles) should be changed according to the local setting. \n",
    "\n",
    "# The matplotlib library seems to need to be installed after the requirements.txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDSbldAH8ya_"
   },
   "source": [
    "# **Install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k1KBrzhDBLuw",
    "outputId": "063d11ac-2b02-41eb-d0c2-ad20b4d6171a"
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "%pip install torchgeo\n",
    "%pip install -U matplotlib\n",
    "%pip install wandb --upgrade -Uqqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdajaldq8JoT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# import torch and neural networks libraries needed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F   # for useful functions (e.g. activation)\n",
    "import torch.optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# import the sklearn metrics needed\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score, r2_score, mean_squared_error\n",
    "\n",
    "# numerical operations without torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# import pandas to read the files\n",
    "import pandas as pd\n",
    "\n",
    "# visualizations\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# to copy e.g. the model from gpu to cpu, etc.\n",
    "import copy\n",
    "\n",
    "# to split the training data into train/val and to get data mini-batches\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# tracking of experiments\n",
    "from tqdm import trange, tqdm\n",
    "import wandb\n",
    "\n",
    "\n",
    "\n",
    "# Fix seeds for reproducibility\n",
    "SEED = 121997\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqMwBkQzrwog"
   },
   "source": [
    "# **Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eICQVfahRzyy"
   },
   "source": [
    "### **Load datasets from files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0WEZBEMRr04",
    "outputId": "0d73cc58-2b96-4aea-fe5e-6153c59c0dcf"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# dataset_field_data_path = \"/content/drive/MyDrive/ETHZ/AI4Good/final_dataset.csv\"\n",
    "# dataset_tiles_path = \"/content/drive/MyDrive/ETHZ/AI4Good/tiles/\"\n",
    "\n",
    "dataset_field_data_path = \"./data/mapping/final_dataset.csv\"\n",
    "dataset_tiles_path = \"./data/tiles/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 939
    },
    "collapsed": true,
    "id": "BSxZfa56WSoF",
    "outputId": "e18d2500-2ce7-4867-9ff2-682da492dccb"
   },
   "outputs": [],
   "source": [
    "dataset_field = pd.read_csv(dataset_field_data_path)\n",
    "dataset_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "N0FryeyYjjjd",
    "outputId": "9dfab076-1808-4a16-a4bb-4a31e39f49b0"
   },
   "outputs": [],
   "source": [
    "# Show all the column names in the dataset\n",
    "print(dataset_field.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyT3j1vIiQyt"
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['img_name', 'year',\n",
    "       'tile_index', 'tile_xmin', 'tile_ymin', 'tile_xmax', 'tile_ymax', 'x',\n",
    "       'y', 'Xmin', 'Ymin', 'Xmax', 'Ymax', 'X_d', 'Y_d',\n",
    "       'is_musacea_d', 'plot_id', 'tree_id', 'is_musacea_g',\n",
    "       'X_g', 'Y_g', 'group', 'id']\n",
    "\n",
    "dataset_field = dataset_field.drop(columns=columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C84dKu2TR-Pp"
   },
   "outputs": [],
   "source": [
    "# Rename the sites to avoid having spaces in the values\n",
    "dataset_field[\"site\"] = dataset_field[\"site\"].apply(lambda x: x.lower())\n",
    "dataset_field[\"site\"] = dataset_field[\"site\"].apply(lambda x: x.replace(\" \", \"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "M94-DhpPg0x4",
    "outputId": "16645353-2af7-4ea0-acb9-fec6e1cf7072"
   },
   "outputs": [],
   "source": [
    "# Show how a sample looks like\n",
    "example_sample = dataset_field.iloc[0]  # choose the first sample\n",
    "print(example_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "oTSMQitbgg-r",
    "outputId": "75c75d11-08aa-4bfb-f4d2-871eacfc5d41"
   },
   "outputs": [],
   "source": [
    "# Print all the different sites' names\n",
    "print(dataset_field[\"site\"].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with different pre-processing for the preparation of the sampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with some pre-processing for \"extreme\" settings\n",
    "\n",
    "# banana = dataset_field[dataset_field[\"name\"] == \"Musacea\"]\n",
    "# banana_mid_point = len(banana) // 2\n",
    "# banana_agb = np.sort(banana['AGB'])\n",
    "# banana = banana[(banana['AGB'] >= banana_agb[banana_mid_point - 300]) & (banana['AGB'] <= banana_agb[banana_mid_point + 300])].reset_index(drop=True).iloc[:600]\n",
    "\n",
    "# non_banana = dataset_field.loc[dataset_field[\"name\"] == \"Cacao\"]\n",
    "# non_banana_mid_point = len(non_banana) // 2\n",
    "# non_banana_agb = np.sort(non_banana['AGB'])\n",
    "# non_banana = non_banana[(non_banana['AGB'] >= non_banana_agb[non_banana_mid_point - 300]) & (non_banana['AGB'] <= non_banana_agb[non_banana_mid_point + 300])].reset_index(drop=True).iloc[:600]\n",
    "\n",
    "# sampled_dataset = pd.concat([banana, non_banana])\n",
    "# sampled_dataset = sampled_dataset.iloc[np.random.permutation(len(sampled_dataset))].reset_index(drop=True)\n",
    "# sampled_dataset\n",
    "\n",
    "# ---\n",
    "\n",
    "# banana = dataset_field[dataset_field[\"is_banana\"] == True]\n",
    "# banana_agb_max = np.sort(banana['AGB'])[600]\n",
    "# banana = banana[banana['AGB'] <= banana_agb_max].reset_index(drop=True).iloc[:600]\n",
    "\n",
    "# non_banana = dataset_field.loc[dataset_field[\"is_banana\"] == False]\n",
    "# non_banana_agb_max = np.sort(non_banana['AGB'])[600]\n",
    "# non_banana = non_banana[non_banana['AGB'] <= non_banana_agb_max].reset_index(drop=True).iloc[:600]\n",
    "\n",
    "# sampled_dataset = pd.concat([banana, non_banana])\n",
    "# sampled_dataset = sampled_dataset.iloc[np.random.permutation(len(sampled_dataset))].reset_index(drop=True)\n",
    "# sampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Removing the first and last nr_outliers_at_end\n",
    "# nr_outliers_at_end = 200\n",
    "# min_val, max_val = np.sort(dataset_field['AGB'])[[nr_outliers_at_end, -nr_outliers_at_end]]\n",
    "# dataset_field = dataset_field[(dataset_field['AGB'] > min_val) & (dataset_field['AGB'] < max_val)].reset_index(drop=True)\n",
    "# print(f'{min_val=}, {max_val=}')\n",
    "# dataset_field"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual paper's pre-processing for the preparation of the sampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qR0VXki5bMBP",
    "outputId": "cf580510-27f7-4707-cba0-b92005075805"
   },
   "outputs": [],
   "source": [
    "# Create subsets of the dataset (with full union) per site\n",
    "carlos_vera_guevara_site_dataset = dataset_field.loc[dataset_field[\"site\"] == \"carlos_vera_guevara_rgb\"]\n",
    "carlos_vera_arteaga_site_dataset = dataset_field.loc[dataset_field[\"site\"] == \"carlos_vera_arteaga_rgb\"]\n",
    "flora_pluas_site_dataset = dataset_field.loc[dataset_field[\"site\"] == \"flora_pluas_rgb\"]\n",
    "leonor_aspiazu_site_dataset = dataset_field.loc[dataset_field[\"site\"] == \"leonor_aspiazu_rgb\"]\n",
    "manuel_macias_site_dataset = dataset_field.loc[dataset_field[\"site\"] == \"manuel_macias_rgb\"]\n",
    "nestor_macias_site_dataset = dataset_field.loc[dataset_field[\"site\"] == \"nestor_macias_rgb\"]\n",
    "\n",
    "print(carlos_vera_guevara_site_dataset.shape)\n",
    "print(carlos_vera_arteaga_site_dataset.shape)\n",
    "print(flora_pluas_site_dataset.shape)\n",
    "print(leonor_aspiazu_site_dataset.shape)\n",
    "print(manuel_macias_site_dataset.shape)\n",
    "print(nestor_macias_site_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_qWoah3t2eu"
   },
   "outputs": [],
   "source": [
    "# Create two subsets per site for banana/non-banana from the site subsets\n",
    "banana_carlos_vera_guevara_site_dataset = carlos_vera_guevara_site_dataset.loc[carlos_vera_guevara_site_dataset[\"is_banana\"] == True]\n",
    "non_banana_carlos_vera_guevara_site_dataset = carlos_vera_guevara_site_dataset.loc[carlos_vera_guevara_site_dataset[\"is_banana\"] == False]\n",
    "\n",
    "banana_carlos_vera_arteaga_site_dataset = carlos_vera_guevara_site_dataset.loc[carlos_vera_guevara_site_dataset[\"is_banana\"] == True]\n",
    "non_banana_carlos_vera_arteaga_site_dataset = carlos_vera_guevara_site_dataset.loc[carlos_vera_guevara_site_dataset[\"is_banana\"] == False]\n",
    "\n",
    "banana_flora_pluas_site_dataset = flora_pluas_site_dataset.loc[flora_pluas_site_dataset[\"is_banana\"] == True]\n",
    "non_banana_flora_pluas_site_dataset = flora_pluas_site_dataset.loc[flora_pluas_site_dataset[\"is_banana\"] == False]\n",
    "\n",
    "banana_leonor_aspiazu_site_dataset = leonor_aspiazu_site_dataset.loc[leonor_aspiazu_site_dataset[\"is_banana\"] == True]\n",
    "non_banana_leonor_aspiazu_site_dataset = leonor_aspiazu_site_dataset.loc[leonor_aspiazu_site_dataset[\"is_banana\"] == False]\n",
    "\n",
    "banana_manuel_macias_site_dataset = manuel_macias_site_dataset.loc[manuel_macias_site_dataset[\"is_banana\"] == True]\n",
    "non_banana_manuel_macias_site_dataset = manuel_macias_site_dataset.loc[manuel_macias_site_dataset[\"is_banana\"] == False]\n",
    "\n",
    "banana_nestor_macias_site_dataset = nestor_macias_site_dataset.loc[nestor_macias_site_dataset[\"is_banana\"] == True]\n",
    "non_banana_nestor_macias_site_dataset = nestor_macias_site_dataset.loc[nestor_macias_site_dataset[\"is_banana\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zyI5W5pTv0Fi",
    "outputId": "23780312-0d39-4019-8e4f-f5625f024309"
   },
   "outputs": [],
   "source": [
    "print(banana_carlos_vera_guevara_site_dataset.shape)\n",
    "print(non_banana_carlos_vera_guevara_site_dataset.shape)\n",
    "\n",
    "print(banana_carlos_vera_arteaga_site_dataset.shape)\n",
    "print(non_banana_carlos_vera_arteaga_site_dataset.shape)\n",
    "\n",
    "print(banana_flora_pluas_site_dataset.shape)\n",
    "print(non_banana_flora_pluas_site_dataset.shape)\n",
    "\n",
    "print(banana_leonor_aspiazu_site_dataset.shape)\n",
    "print(non_banana_leonor_aspiazu_site_dataset.shape)\n",
    "\n",
    "print(banana_manuel_macias_site_dataset.shape)\n",
    "print(non_banana_manuel_macias_site_dataset.shape)\n",
    "\n",
    "print(banana_nestor_macias_site_dataset.shape)\n",
    "print(non_banana_nestor_macias_site_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vXwa8-vDbMLi",
    "outputId": "b9cf8280-0979-4279-84aa-18e02f04f6e9"
   },
   "outputs": [],
   "source": [
    "# We can sample 100 data samples from each of the subsets (pandas dataframes) created\n",
    "\n",
    "sampled_df1 = banana_carlos_vera_guevara_site_dataset.sample(n=100)\n",
    "sampled_df2 = non_banana_carlos_vera_guevara_site_dataset.sample(n=100)\n",
    "sampled_df3 = banana_carlos_vera_arteaga_site_dataset.sample(n=100)\n",
    "sampled_df4 = non_banana_carlos_vera_arteaga_site_dataset.sample(n=100)\n",
    "sampled_df5 = banana_flora_pluas_site_dataset.sample(n=100)\n",
    "sampled_df6 = non_banana_flora_pluas_site_dataset.sample(n=100)\n",
    "sampled_df7 = banana_leonor_aspiazu_site_dataset.sample(n=100)\n",
    "sampled_df8 = non_banana_leonor_aspiazu_site_dataset.sample(n=100)\n",
    "sampled_df9 = banana_manuel_macias_site_dataset.sample(n=100)\n",
    "sampled_df10 = non_banana_manuel_macias_site_dataset.sample(n=100)\n",
    "sampled_df11 = banana_nestor_macias_site_dataset.sample(n=100)\n",
    "sampled_df12 = non_banana_nestor_macias_site_dataset.sample(n=100)\n",
    "\n",
    "print(sampled_df1.shape)\n",
    "print(sampled_df2.shape)\n",
    "print(sampled_df3.shape)\n",
    "print(sampled_df4.shape)\n",
    "print(sampled_df5.shape)\n",
    "print(sampled_df6.shape)\n",
    "print(sampled_df7.shape)\n",
    "print(sampled_df8.shape)\n",
    "print(sampled_df9.shape)\n",
    "print(sampled_df10.shape)\n",
    "print(sampled_df11.shape)\n",
    "print(sampled_df12.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-qkhJxVa-3_"
   },
   "outputs": [],
   "source": [
    "# Create the fully sampled dataset for our experiment (1200 trees as per the paper)\n",
    "sampled_dataset = pd.concat([sampled_df1, sampled_df2, sampled_df3, sampled_df4, sampled_df5, sampled_df6, sampled_df7, sampled_df8, sampled_df9, sampled_df10, sampled_df11, sampled_df12])\n",
    "sampled_dataset = sampled_dataset.iloc[np.random.permutation(len(sampled_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "95iK-H1Na-6i",
    "outputId": "070835e0-dcfd-4da0-fe39-c9678233ce9a"
   },
   "outputs": [],
   "source": [
    "# Create the train-val-test datasets from the above dataset by sampling randomly\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(sampled_dataset, test_size=0.3, random_state=SEED)\n",
    "train_df, test_df = train_test_split(train_df, test_size=0.1, random_state=SEED)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "h0qzJt2TYDD-",
    "outputId": "d395126e-0301-4e46-c83c-93922461fe76"
   },
   "outputs": [],
   "source": [
    "print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hz5TkTzcq6f6"
   },
   "source": [
    "### **Extract the individual trees from the bounding boxes & Prepare the train-val-test lists of data (i.e., image trees, agb values, tree group labels)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMZmL22BYDGa"
   },
   "outputs": [],
   "source": [
    "nn_input_image_size = 800   # choice from the ReforesTree paper to make sure to match bigger tree images too\n",
    "\n",
    "def bbox_float_to_int(bbox):\n",
    "    return [round(coordinate) for coordinate in bbox]\n",
    "\n",
    "def reshape_bbox(tree_image):\n",
    "    W_padding = nn_input_image_size - tree_image.shape[2]   # width padding  \n",
    "    H_padding = nn_input_image_size - tree_image.shape[1]   # height padding\n",
    "\n",
    "    left_W_padding = W_padding // 2\n",
    "    right_W_padding = W_padding // 2\n",
    "    top_H_padding = H_padding // 2\n",
    "    bottom_H_padding = H_padding // 2\n",
    "\n",
    "    if tree_image.shape[2] + left_W_padding + right_W_padding == nn_input_image_size - 1:\n",
    "        right_W_padding = right_W_padding + 1\n",
    "    if tree_image.shape[1] + top_H_padding + bottom_H_padding == nn_input_image_size - 1:\n",
    "        bottom_H_padding = bottom_H_padding + 1\n",
    "    \n",
    "    # See: https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html\n",
    "    p2d = (left_W_padding, right_W_padding, top_H_padding, bottom_H_padding)  # pad the left side and right side each by 10, pad the upper side and lower side each by 20\n",
    "    \n",
    "    padded_tree_image = F.pad(tree_image, p2d, \"constant\", 0)\n",
    "\n",
    "    return padded_tree_image\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def get_tree_from_image(sample):\n",
    "    image_path_i = sample['img_path']\n",
    "    index = image_path_i.find(\"_\")\n",
    "    site_folder_name = image_path_i[:index]\n",
    "    image_path_i = dataset_tiles_path + site_folder_name + \"/\" + sample['img_path']     # create the full path to get the image\n",
    "    \n",
    "    bbox = [sample['xmin'], sample['ymin'], sample['xmax'], sample['ymax']]     # store the four components of the bbox in a list \n",
    "    # print(bbox)\n",
    "    _num_channels = 3   # RGB image is considered\n",
    "\n",
    "    \n",
    "    # print(\"Getting the tree at image file location: \", image_path_i)\n",
    "    image = Image.open(image_path_i)\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    image = to_tensor(image)    # make the image a Tensor\n",
    "\n",
    "    bbox = bbox_float_to_int(bbox)    # cast coordinates of bbox from float to int    \n",
    "    \n",
    "    tree_binary_label = sample['is_banana']    # i.e., the binary group banana/non-banana\n",
    "    tree_agb_value = sample['AGB']\n",
    "\n",
    "    tree_image = image[:, int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]     # crop the image to bounding box\n",
    "    # plt.imshow((tree_image).permute(1, 2, 0))\n",
    "\n",
    "    tree_image_reshaped = reshape_bbox(tree_image)      # fit the cropped image to shape of 800x800\n",
    "\n",
    "    tree_image_reshaped_normalized = tree_image_reshaped    # normalize the pixel values    ??? We do NOT normalize??? We get black pixels when we divide by 255...\n",
    "\n",
    "    return tree_image_reshaped_normalized.to(torch.float), tree_binary_label, tree_agb_value.astype(dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ggrgYLNsYDxl",
    "outputId": "edb183a9-4d14-4bde-ffcf-1703261c7088"
   },
   "outputs": [],
   "source": [
    "## Sample the trees from the image samples\n",
    "\n",
    "def sample_trees(df, df_mode, set_nb_of_samples=None):\n",
    "\n",
    "    tree_images = []\n",
    "    tree_labels = []\n",
    "    tree_agb_values = []\n",
    "\n",
    "    total_number_of_trees = 0\n",
    "\n",
    "    if set_nb_of_samples is None:\n",
    "        set_nb_of_samples = df.shape[0]\n",
    "\n",
    "    print(f\"Total number of tree samples for {df_mode}: \", set_nb_of_samples)\n",
    "\n",
    "    for i in range(set_nb_of_samples):\n",
    "        sample = df.iloc[i]        \n",
    "        tree_i, label_i, agb_value_i = get_tree_from_image(sample)\n",
    "        tree_images.append(tree_i)\n",
    "        tree_labels.append(label_i)\n",
    "        tree_agb_values.append(agb_value_i)\n",
    "\n",
    "    return tree_images, tree_labels, tree_agb_values\n",
    "\n",
    "\n",
    "# /!\\ Do not forget to set set_nb_of_samples=None if we want to use the full sampled dataset\n",
    "train_tree_images, train_tree_labels, train_tree_agb_values = sample_trees(train_df, df_mode=\"train\", set_nb_of_samples=None)\n",
    "print(\"Done with getting the trees for training\")\n",
    "val_tree_images, val_tree_labels, val_tree_agb_values = sample_trees(val_df, df_mode=\"val\", set_nb_of_samples=None)\n",
    "print(\"Done with getting the trees for validation\")\n",
    "test_tree_images, test_tree_labels, test_tree_agb_values = sample_trees(test_df, df_mode=\"test\", set_nb_of_samples=None)\n",
    "print(\"Done with getting the trees for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5xc-ylW2urC"
   },
   "outputs": [],
   "source": [
    "# print(train_tree_images)\n",
    "# print(train_tree_labels)\n",
    "# print(train_tree_agb_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "udG6LpFej51J"
   },
   "source": [
    "### **Check the state of our data until here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "czY_rUONnvyu",
    "outputId": "dd663a8e-2e58-4660-b2c5-25b36e6c1ded"
   },
   "outputs": [],
   "source": [
    "# Print the results of our pre-processing to get lists with all the samples needed\n",
    "print(train_tree_images)\n",
    "print(train_tree_labels)\n",
    "print(train_tree_agb_values)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "jclEudpRL2nq",
    "outputId": "abc47a14-eca7-42c2-c967-40b53c47ce89"
   },
   "outputs": [],
   "source": [
    "print(f\"List of TRAIN {len(train_tree_images)} tree samples represented as tensors of shape {train_tree_images[0].shape}\")\n",
    "print(\"Total number of binary labels (i.e., banana/non-banana): \", len(train_tree_labels))\n",
    "print(\"Total number of AGB values: \", len(train_tree_agb_values))\n",
    "\n",
    "print(f\"List of VAL {len(val_tree_images)} tree samples represented as tensors of shape {val_tree_images[0].shape}\")\n",
    "print(\"Total number of binary labels (i.e., banana/non-banana): \", len(val_tree_labels))\n",
    "print(\"Total number of AGB values: \", len(val_tree_agb_values))\n",
    "\n",
    "print(f\"List of TEST {len(test_tree_images)} tree samples represented as tensors of shape {test_tree_images[0].shape}\")\n",
    "print(\"Total number of binary labels (i.e., banana/non-banana): \", len(test_tree_labels))\n",
    "print(\"Total number of AGB values: \", len(test_tree_agb_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBzltiJ8pRnd"
   },
   "source": [
    "### **One-Hot Encode (OHE) the tree group labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Z_Li6D4GYD0I",
    "outputId": "d27aa46d-18ca-40d7-e20f-298210835c44"
   },
   "outputs": [],
   "source": [
    "## OHE the tree group label values\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "## Training\n",
    "values = array(train_tree_labels)\n",
    "train_label_encoder = LabelEncoder()\n",
    "integer_encoded = train_label_encoder.fit_transform(values)\n",
    "train_onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "train_onehot_encoded = train_onehot_encoder.fit_transform(integer_encoded)\n",
    "# inverted = train_label_encoder.inverse_transform([argmax(train_onehot_encoded[0, :])])\n",
    "# print(inverted)\n",
    "\n",
    "train_labels_list = []\n",
    "for i in range(train_onehot_encoded.shape[0]):\n",
    "    train_labels_list.append(torch.Tensor(train_onehot_encoded[i, :]))\n",
    "\n",
    "print(\"One Hot Encoded (OHE) tree group labels for TRAINING: \\n\", train_labels_list)\n",
    "\n",
    "\n",
    "## Validation\n",
    "values = array(val_tree_labels)\n",
    "val_label_encoder = LabelEncoder()\n",
    "integer_encoded = val_label_encoder.fit_transform(values)\n",
    "val_onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "val_onehot_encoded = val_onehot_encoder.fit_transform(integer_encoded)\n",
    "# inverted = val_label_encoder.inverse_transform([argmax(val_onehot_encoded[0, :])])\n",
    "# print(inverted)\n",
    "\n",
    "val_labels_list = []\n",
    "for i in range(val_onehot_encoded.shape[0]):\n",
    "    val_labels_list.append(torch.Tensor(val_onehot_encoded[i, :]))\n",
    "\n",
    "print(\"One Hot Encoded (OHE) tree group labels for VALIDATION: \\n\", val_labels_list)\n",
    "\n",
    "\n",
    "## Testing\n",
    "values = array(test_tree_labels)\n",
    "test_label_encoder = LabelEncoder()\n",
    "integer_encoded = test_label_encoder.fit_transform(values)\n",
    "test_onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "test_onehot_encoded = test_onehot_encoder.fit_transform(integer_encoded)\n",
    "# inverted = test_label_encoder.inverse_transform([argmax(test_onehot_encoded[0, :])])\n",
    "# print(inverted)\n",
    "\n",
    "test_labels_list = []\n",
    "for i in range(test_onehot_encoded.shape[0]):\n",
    "    test_labels_list.append(torch.Tensor(test_onehot_encoded[i, :]))\n",
    "\n",
    "# print(\"One Hot Encoded (OHE) tree group labels for TESTING: \\n\", test_labels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIeGjVP5oPm7"
   },
   "source": [
    "### **See what are the unique values taken by our feature(s) and target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Qr_T_E6UYN6G",
    "outputId": "f314d6fe-d37d-4e16-a47b-051c3f0eb878"
   },
   "outputs": [],
   "source": [
    "## Count how many trees per binary group of trees we have in our training set, validation set and testing set.\n",
    "## It should be quite balanced.\n",
    "# Note: \n",
    "# element 0 is non-banana\n",
    "# element 1 is banana\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "## For TRAINING\n",
    "print(\"\\n\", train_labels_list)\n",
    "train_tree_labels_list = [ohe_label for ohe_label in train_labels_list]\n",
    "\n",
    "# inverted = train_label_encoder.inverse_transform([argmax(train_onehot_encoded[0, :])]).tolist()\n",
    "# print(inverted)\n",
    "\n",
    "train_original_labels = [train_label_encoder.inverse_transform([argmax(train_onehot_encoded[i, :])]).tolist()[0] for i in range(len(train_tree_labels_list))]\n",
    "print(train_original_labels)\n",
    "print(Counter(train_original_labels).keys())\n",
    "print(Counter(train_original_labels).values())\n",
    "\n",
    "\n",
    "## For VALIDATION\n",
    "print(\"\\n\", val_labels_list)\n",
    "val_tree_labels_list = [ohe_label for ohe_label in val_labels_list]\n",
    "\n",
    "# inverted = val_label_encoder.inverse_transform([argmax(val_onehot_encoded[0, :])]).tolist()\n",
    "# print(inverted)\n",
    "\n",
    "val_original_labels = [val_label_encoder.inverse_transform([argmax(val_onehot_encoded[i, :])]).tolist()[0] for i in range(len(val_tree_labels_list))]\n",
    "print(val_original_labels)\n",
    "print(Counter(val_original_labels).keys())\n",
    "print(Counter(val_original_labels).values())\n",
    "\n",
    "## For TESTING\n",
    "print(\"\\n\", test_labels_list)\n",
    "test_tree_labels_list = [ohe_label for ohe_label in test_labels_list]\n",
    "\n",
    "# inverted = test_label_encoder.inverse_transform([argmax(test_onehot_encoded[0, :])]).tolist()\n",
    "# print(inverted)\n",
    "\n",
    "test_original_labels = [test_label_encoder.inverse_transform([argmax(test_onehot_encoded[i, :])]).tolist()[0] for i in range(len(test_tree_labels_list))]\n",
    "# print(test_original_labels)\n",
    "print(Counter(test_original_labels).keys())\n",
    "print(Counter(test_original_labels).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "vGT1qpg7YZ8D",
    "outputId": "429d6780-946d-452c-99b9-7119dcee42a6"
   },
   "outputs": [],
   "source": [
    "## Count how many different AGB values we have among all the trees sampled. Note that I use Counter() because I assumed that there were not many different values\n",
    "## We can see that although it is a regression problem, there are NOT many AGB values\n",
    "\n",
    "## For TRAINING\n",
    "train_tree_agb_values_list = [tensor_agb_value.item() for tensor_agb_value in train_tree_agb_values]\n",
    "print(Counter(train_tree_agb_values_list).keys())\n",
    "print(Counter(train_tree_agb_values_list).values())\n",
    "\n",
    "print(len(Counter(train_tree_agb_values_list).values()))\n",
    "\n",
    "## For VALIDATION\n",
    "val_tree_agb_values_list = [tensor_agb_value.item() for tensor_agb_value in val_tree_agb_values]\n",
    "print(Counter(val_tree_agb_values_list).keys())\n",
    "print(Counter(val_tree_agb_values_list).values())\n",
    "\n",
    "print(len(Counter(val_tree_agb_values_list).values()))\n",
    "\n",
    "## For TESTING\n",
    "test_tree_agb_values_list = [tensor_agb_value.item() for tensor_agb_value in test_tree_agb_values]\n",
    "print(Counter(test_tree_agb_values_list).keys())\n",
    "print(Counter(test_tree_agb_values_list).values())\n",
    "\n",
    "print(len(Counter(test_tree_agb_values_list).values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzjZZrxSoCCx"
   },
   "source": [
    "### **Summary of the data (train, validation, test)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Ggui3xpGYZ-p",
    "outputId": "2992d8b7-7e29-4304-d7f5-3f49746cd2df"
   },
   "outputs": [],
   "source": [
    "# For TRAINING\n",
    "print(\"Total number of trees for TRAINING: \", len(train_tree_images))\n",
    "print(train_tree_images[0])\n",
    "print(train_tree_images[0].shape)\n",
    "\n",
    "# Check all the information we have for one sample\n",
    "print(train_tree_images[0])\n",
    "print(train_labels_list[0])   # recall: OHE\n",
    "print(train_tree_agb_values[0])\n",
    "\n",
    "print(train_tree_images[0].shape)\n",
    "print(train_labels_list[0])\n",
    "print(\"A OHE label shape for TRAINING: \", train_labels_list[0].shape)\n",
    "\n",
    "\n",
    "## For VALIDATION\n",
    "print(\"Total number of trees for VALIDATION: \", len(val_tree_images))\n",
    "print(val_tree_images[0])\n",
    "print(val_tree_images[0].shape)\n",
    "\n",
    "# Check all the information we have for one sample\n",
    "print(val_tree_images[0])\n",
    "print(val_labels_list[0])   # recall: OHE\n",
    "print(val_tree_agb_values[0])\n",
    "\n",
    "print(val_tree_images[0].shape)\n",
    "print(val_labels_list[0])\n",
    "print(\"A OHE label shape for VALIDATION: \", val_labels_list[0].shape)\n",
    "\n",
    "\n",
    "## For TESTING\n",
    "print(\"Total number of trees for TESTING: \", len(test_tree_images))\n",
    "print(test_tree_images[0])\n",
    "print(test_tree_images[0].shape)\n",
    "\n",
    "# Check all the information we have for one sample\n",
    "print(test_tree_images[0])\n",
    "print(test_labels_list[0])   # recall: OHE\n",
    "print(test_tree_agb_values[0])\n",
    "\n",
    "print(test_tree_images[0].shape)\n",
    "print(test_labels_list[0])\n",
    "print(\"A OHE label shape for TESTING: \", test_labels_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uvj0bEL269oX"
   },
   "outputs": [],
   "source": [
    "# Just naming of variables for convenience\n",
    "train_tree_labels = train_labels_list\n",
    "val_tree_labels = val_labels_list\n",
    "test_tree_labels = test_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddidY_EZBOti"
   },
   "outputs": [],
   "source": [
    "# Sanity check to see if the tree image tensors are all NOT empty\n",
    "\n",
    "for i in range(len(train_tree_images)):\n",
    "    flat_tensor = train_tree_images[i].flatten()\n",
    "    if flat_tensor.all():\n",
    "        print(\"All values in the tensor are 0...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbLR8iUckPyK"
   },
   "source": [
    "### **Plot some samples to see the tree images that we will feed to the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "collapsed": true,
    "id": "E4H8wY_5R7YH",
    "outputId": "754ba23e-f38f-44ff-abcf-54a2d31d7663"
   },
   "outputs": [],
   "source": [
    "print(train_tree_images[0].permute(1, 2, 0).shape)\n",
    "plt.imshow(train_tree_images[0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "collapsed": true,
    "id": "xOnX07mzR7pM",
    "outputId": "4fc03423-edc1-4a94-df69-34f36e4facdb"
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_tree_images[1].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "collapsed": true,
    "id": "Kt-1XO2aR7sf",
    "outputId": "b81f3334-d967-4e96-ed0a-eb4e72c8d750"
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_tree_images[2].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "collapsed": true,
    "id": "tmBjjs4Iv4DW",
    "outputId": "a5426736-0d06-442b-a077-049acebae4ee"
   },
   "outputs": [],
   "source": [
    "plt.imshow(val_tree_images[0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "collapsed": true,
    "id": "JJE2464wv43m",
    "outputId": "b2403053-0ed2-46cd-8608-b61df347faa0"
   },
   "outputs": [],
   "source": [
    "plt.imshow(val_tree_images[1].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "collapsed": true,
    "id": "KPBflAY5v5Fa",
    "outputId": "e3b15a8c-5a2b-47d2-acbe-7692545a628c"
   },
   "outputs": [],
   "source": [
    "plt.imshow(val_tree_images[2].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "collapsed": true,
    "id": "wC1HcJl8v8Xc",
    "outputId": "62dd4da5-5723-45df-915a-5ac0ebfc2cf3"
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_tree_images[0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "collapsed": true,
    "id": "QAHkjEXNv8Ze",
    "outputId": "5f01ea7d-c3a1-4881-fc43-a9de5a7803d5"
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_tree_images[1].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "collapsed": true,
    "id": "VJdB-qztv8cS",
    "outputId": "b10e1a23-d414-4644-de72-9a90619bf188"
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_tree_images[2].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgYy1EAmwjJ1"
   },
   "source": [
    "### **Prepare data to train and evaluate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAli99Mmwjea"
   },
   "outputs": [],
   "source": [
    "from torch._C import dtype\n",
    "\n",
    "# Create a PyTorch class MyDataset in order to obtain a whole dataset with X (images) AND Y data (agb values) (and X features inputs in addition to the images input)\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X_data, Y_data, X_train_features, transform=None):\n",
    "\n",
    "        # the arguments (images and targets and additional features) should all be a list of tensors\n",
    "\n",
    "        self.tree_images = X_data\n",
    "        self.tree_abg_values = Y_data\n",
    "        self.tree_groups = X_train_features\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tree_abg_values)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # print(type(self.tree_images[idx]))\n",
    "        # print(type(self.tree_abg_values[idx]))\n",
    "        # print(type(self.tree_groups[idx]))\n",
    "\n",
    "        # self.tree_images[idx] = self.tree_images[idx].to(torch.float)\n",
    "        # self.tree_abg_values[idx] = self.tree_abg_values[idx].astype(dtype=np.float32)\n",
    "        self.tree_groups[idx] = self.tree_groups[idx].to(torch.float)\n",
    "\n",
    "        # print(self.tree_images[idx].type())\n",
    "        # print(type(self.tree_abg_values[idx]))\n",
    "        # print(self.tree_groups[idx].type())\n",
    "        \n",
    "        sample = self.tree_images[idx], self.tree_abg_values[idx], self.tree_groups[idx]\n",
    "\n",
    "        # print(self.tree_images[idx])\n",
    "        # print(self.tree_abg_values[idx])\n",
    "        # print(self.tree_groups[idx])\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(self.tree_images[idx]), self.tree_abg_values[idx], self.tree_groups[idx]\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiC-FGwyH3m4"
   },
   "outputs": [],
   "source": [
    "# If using the folder data (as opposed to the torchgeo dataset)\n",
    "\n",
    "X_train_images, Y_train, X_train_features = train_tree_images, train_tree_agb_values, train_tree_labels\n",
    "X_val_images, Y_val, X_val_features = val_tree_images, val_tree_agb_values, val_tree_labels, \n",
    "X_test_images, Y_test, X_test_features = test_tree_images, test_tree_agb_values, test_tree_labels\n",
    "\n",
    "del train_tree_images\n",
    "del train_tree_labels\n",
    "del train_tree_agb_values\n",
    "del val_tree_images\n",
    "del val_tree_labels\n",
    "del val_tree_agb_values\n",
    "del test_tree_images\n",
    "del test_tree_labels\n",
    "del test_tree_agb_values\n",
    "\n",
    "my_transform = transforms.Compose([\n",
    "        transforms.Resize([224,224]), # resize the image as the ResNet18 needs 224 x 244 as input dimensions\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        # transforms.ToTensor(),  # not needed as we already have a Tensor (e.g., not a ndarray)\n",
    "        # transforms.Normalize(mean=(0.25,0.25,0.25), std=(0.14,0.14,0.14))\n",
    "    ])\n",
    "\n",
    "train_dataset = MyDataset(X_train_images, Y_train, X_train_features, transform=my_transform)\n",
    "val_dataset = MyDataset(X_val_images, Y_val, X_val_features, transform=my_transform)\n",
    "test_dataset = MyDataset(X_test_images, Y_test, X_test_features, transform=my_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhUB59k2M3Tw",
    "outputId": "02167599-ed21-4db1-a54d-66086b011c45"
   },
   "outputs": [],
   "source": [
    "del X_train_images\n",
    "del Y_train\n",
    "del X_train_features\n",
    "del X_val_images\n",
    "del Y_val\n",
    "del X_val_features\n",
    "del X_test_images\n",
    "del Y_test\n",
    "del X_test_features\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSK5I41vS5or"
   },
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmLBrjmJr5nP"
   },
   "source": [
    "### **Define the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpoVPXWeS5-i"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "## Define a simple CNN baseline model\n",
    "class BaselineCNN(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_features: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,              \n",
    "            out_channels=8,            \n",
    "            kernel_size=3,              \n",
    "            stride=1,                   \n",
    "            padding=2,                  \n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=8,              \n",
    "            out_channels=16,            \n",
    "            kernel_size=5,              \n",
    "            stride=1,                   \n",
    "            padding=2,                  \n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(50176, 100)  \n",
    "        self.fc2 = nn.Linear(100, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))        \n",
    "        x = x.view(x.size(0), -1)   # flatten the output of the convolution to (batch_size, ...)   \n",
    "        x = self.relu(self.fc1(x))\n",
    "        output = self.fc2(x)\n",
    "    \n",
    "        return output\n",
    "\n",
    "\n",
    "## Define the fine-tuned model\n",
    "class FineTuneModel(nn.Module):\n",
    "    def __init__(self, out_features: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # ResNet18\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)     # weights=ResNet18_Weights.DEFAULT or weights=ResNet18_Weights.IMAGENET1K_V1\n",
    "        # ResNet50\n",
    "        # self.resnet = models.resnet50(weights=\"IMAGENET1K_V2\")     # weights=ResNet50_Weights.DEFAULT or weights=ResNet50_Weights.IMAGENET1K_V2\n",
    "\n",
    "        # Freeze the weights of all the layers of the ResNet\n",
    "        # for param in self.resnet18.parameters():\n",
    "        #     param.requires_grad = False     \n",
    "        \n",
    "        # Change the last layer of ResNet18 to get an embedding layer to be used later with the other input which together will be mapper to a single value as we want to perform regression\n",
    "        embedd_size = 256   # embedding layer size of the ResNet18 output instead of its usual classification layer; arbitrary\n",
    "        self.resnet.fc = nn.Linear(512, embedd_size)  # 512 for ResNet18; by default, a newly created/modified layer gets requires_grad=True\n",
    "        # self.resnet.fc = nn.Linear(2048, embedd_size)  # 2048 for ResNet50; by default, a newly created/modified layer gets requires_grad=True\n",
    "        \n",
    "        categorical_feature_embedd_size = 2     # e.g., 6 if we have 6 possible group labels (hence OHE vector of size 6), or 2 if we only considered the OHE banana/non-banana\n",
    "        combined_embedd_size = embedd_size + categorical_feature_embedd_size  \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(combined_embedd_size, 128)  # 128 is an arbitrary choice \n",
    "        self.final_layer = nn.Linear(128, out_features)\n",
    "\n",
    "    def forward(self, x_image, x_group_feature_embedded):\n",
    "        # print(x.shape)\n",
    "\n",
    "        # ResNet (encoder) for the image input\n",
    "        x_image_output = self.resnet(x_image)\n",
    "\n",
    "        # Concatenate the output of the ResNet embedding layer with the OHE embedding of the categorical feature (i.e., tree group label) \n",
    "        combined_input = torch.concat([x_image_output, x_group_feature_embedded], dim=1)\n",
    "\n",
    "        # Simple FeedForward part to get the regression output from the combined inputs (i.e., image and (OHE) categorical feature)\n",
    "        output = self.relu(self.fc1(combined_input))\n",
    "        output = self.final_layer(output)\n",
    "    \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1pn9B1vrmpk"
   },
   "source": [
    "### **Instantiate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "R038TcWpDroT",
    "outputId": "e370bdf5-a3fc-4cf0-928b-b957f1d88019"
   },
   "outputs": [],
   "source": [
    "## Create the model\n",
    "\n",
    "## Baseline CNN\n",
    "# self.model = BaselineCNN(in_channels=3, out_features=1)\n",
    "\n",
    "## To directly predict with ResNet18 only\n",
    "# model = models.resnet18(pretrained=True)     # weights=ResNet18_Weights.DEFAULT or weights=ResNet18_Weights.IMAGENET1K_V1\n",
    "# for param in model.parameters():\n",
    "    # param.requires_grad = False     # to freeze the weights of all the layers\n",
    "# model.fc = nn.Linear(512, 1)    # change the last layer to a fully connected one so that the output is single value for regression; the weights have gradient set to True by default, so we keep them as such since we want to train the last layer only!\n",
    "\n",
    "## FineTune ResNet18\n",
    "model = FineTuneModel(out_features=1)   # single target regression problem, hence out_features=1\n",
    "\n",
    "print(model)\n",
    "print(\"\\nThe number of learnable parameters in the model is: \", sum([p.numel() for p in model.parameters() if p.requires_grad]), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSOZQM31sCa1"
   },
   "source": [
    "# **Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda:0'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "print(\"Running on: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoUIglwMS6Cs"
   },
   "outputs": [],
   "source": [
    "class Framework(object):\n",
    "    \"\"\"\n",
    "    General framework for experiments.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_loader, val_loader, criterion, optimizer, batch_size, num_epochs, learning_rate, *args, **kwargs):\n",
    "\n",
    "        self.print_interval = 10    # number of batches to process before displaying the updated metrics during training\n",
    "\n",
    "    # For training\n",
    "    def train(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # For evaluation\n",
    "    def predict(self, eval_loader):\n",
    "        estimations_batches = []\n",
    "        targets_batches = []\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        for (batch_x, batch_y, batch_group_feature_x) in tqdm(eval_loader):\n",
    "            \n",
    "            # estimate/predict the target values\n",
    "            estimated_targets = self.model(batch_x.to(DEVICE), batch_group_feature_x.to(DEVICE))\n",
    "            estimations_batches.append(estimated_targets.cpu().detach().numpy())\n",
    "            \n",
    "            # store the associated targets batch \n",
    "            targets_batches.append(batch_y)\n",
    "\n",
    "        output = np.concatenate(estimations_batches, axis=0)\n",
    "        targets = np.concatenate(targets_batches, axis=0)\n",
    "        \n",
    "        return output, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbck0Kd6UCpJ"
   },
   "outputs": [],
   "source": [
    "class ModelTrainer(Framework):\n",
    "    \"\"\"\n",
    "    The Trainer for the model inheriting the general training-evaluation framework.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_loader, val_loader, criterion, optimizer, batch_size, num_epochs, learning_rate, *args, **kwargs):\n",
    "        super().__init__(model, train_loader, val_loader, criterion, optimizer, batch_size, num_epochs, learning_rate, *args, **kwargs)\n",
    "\n",
    "        # === SETUP ===\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        # self.val_loader = val_loader\n",
    "\n",
    "        # Hyperparameters and general parameters\n",
    "        self.num_epochs = num_epochs # 30\n",
    "        # self.learning_rate = learning_rate # 1e-4\n",
    "        # self.weight_decay_tuned = 5e-3\n",
    "        \n",
    "        # params = filter(lambda p: p.requires_grad, self.model.parameters())     # filter out the parameters to NOT be fine-tuned; allows to specify to the optimizer the weights to update\n",
    "        self.optimizer = optimizer # torch.optim.Adam(params, lr=self.learning_rate)   # self.model.parameters()\n",
    "        # self.lrs = torch.optim.lr_scheduler.StepLR(self.optimizer, 10, 0.7, verbose=True)   # for learning rate scheduling\n",
    "        self.criterion = criterion # nn.MSELoss()\n",
    "\n",
    "        # ===\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        progress_bar = trange(self.num_epochs)\n",
    "        for _ in progress_bar:\n",
    "            for batch_idx, (batch_images_x, batch_y, batch_group_feature_x) in enumerate(self.train_loader):\n",
    "\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                # Set data to gpu if possible\n",
    "                batch_images_x = batch_images_x.to(DEVICE)\n",
    "                batch_y = batch_y.to(DEVICE)\n",
    "                batch_group_feature_x = batch_group_feature_x.to(DEVICE)\n",
    "\n",
    "                # Perform forward pass\n",
    "                estimated_target = self.model(batch_images_x, batch_group_feature_x)\n",
    "\n",
    "                loss = self.criterion(estimated_target, batch_y.unsqueeze(1))   # unsqueeze the batch_y to avoid a warning that could lead to stacking errors; here we had estimated_target of size 64 and batch_y of size 64 x 1\n",
    "                \n",
    "                wandb.log({\"MSE loss (each batch) \": loss.item()})\n",
    "                batch_rmse = mean_squared_error(batch_y.cpu(), estimated_target.cpu().detach().numpy(), squared=False)\n",
    "                wandb.log({f\"RMSE (each batch) \": batch_rmse})\n",
    "\n",
    "                # Backpropagate to get the gradients\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Update progress bar for training every self.print_interval batches\n",
    "                if batch_idx % self.print_interval == 0:\n",
    "                    # output = self.model(batch_images_x, batch_group_feature_x)\n",
    "                    output = estimated_target\n",
    "                    current_rmse = mean_squared_error(batch_y.cpu(), output.cpu().detach().numpy(), squared=False)\n",
    "                    wandb.log({f\"RMSE (of a batch) every {self.print_interval} batches\": batch_rmse})\n",
    "                    progress_bar.set_postfix(loss=loss.item(), current_rmse=batch_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_trainer:Framework, eval_loader):\n",
    "    \"\"\"\n",
    "    :param model: Trained model to be evaluated\n",
    "    :param eval_loader: The validation loader\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the predictions for the whole eval_loader\n",
    "    output, targets = model_trainer.predict(eval_loader)\n",
    "\n",
    "    output = output.flatten()   # to have exactly same size as targets\n",
    "    # targets = np.expand_dims(targets, 1)    # to have exactly same size as output if we did not change the size of targets\n",
    "    \n",
    "    print(\"\\nResult with biggest difference: \", np.max(np.abs(output-targets)))\n",
    "    # print(\"\\n\\nOnly print the first 20 predictions and targets:\\n\")\n",
    "    # print(\"Predictions: \\n\", output[:20])\n",
    "    # print(\"Targets: \\n\", targets[:20])\n",
    "    # print(\"Difference between output and targets: \\n\", output[:20]-targets[:20])\n",
    "    # print(\"Output shape: \", output.shape)\n",
    "    # print(\"Targets shape: \", targets.shape)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    # error_from_scratch = np.sqrt(((output-targets)**2).mean())  # compute RMSE in pure numpy\n",
    "    # print(error_from_scratch)  \n",
    "    error = mean_squared_error(targets, output, squared=False)      # with squared=False we get the RMSE (as in the paper)\n",
    "\n",
    "    print(\"--- Evaluation ---\")\n",
    "    print(f'Root Mean Squared Error: {error:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_solution(model, train_loader, val_loader, criterion, optimizer, batch_size=32, num_epochs=20, learning_rate=1e-4):\n",
    "    if val_loader is None:\n",
    "        val_loader = train_loader\n",
    "\n",
    "    trainer = ModelTrainer(model, train_loader, val_loader, criterion, optimizer, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate)\n",
    "    trainer.train()\n",
    "\n",
    "    if val_loader is None:\n",
    "        print('Evaluating model on TRAINING data (and NOT on validation data) !')\n",
    "\n",
    "    # evaluate(trainer, eval_loader=val_loader)\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rm7LJ0iMsbl"
   },
   "outputs": [],
   "source": [
    "# Define the main hyperparameters\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-4\n",
    "N_EPOCHS = 30  # before 2000\n",
    "\n",
    "params = filter(lambda p: p.requires_grad, model.parameters())   # filter out the parameters to NOT be fine-tuned; allows to specify to the optimizer the weights to update\n",
    "OPTIMIZER = torch.optim.Adam(params, lr=LR)\n",
    "CRITERION = nn.MSELoss()      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgkb1RJtMmmT",
    "outputId": "87f1bf81-c152-4ec1-8299-eabd1b7d6135"
   },
   "outputs": [],
   "source": [
    "# Create and save the three dataloaders\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=2)     # use dataset_train to check if learning for now (????)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=2)     # use dataset_train to check if learning for now (????)\n",
    "\n",
    "# del train_dataset\n",
    "# del val_dataset\n",
    "# del test_dataset\n",
    "\n",
    "# Save the newly created dataloaders\n",
    "!mkdir dataloaders\n",
    "torch.save(train_loader, './dataloaders/train_dataloader.pt')\n",
    "torch.save(val_loader, './dataloaders/val_dataloader.pt')\n",
    "torch.save(test_loader, './dataloaders/test_dataloader.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5jHT1DzMtQK"
   },
   "outputs": [],
   "source": [
    "# Zip the dataloader folder in which we saved the created dataloaders\n",
    "\n",
    "# zip the folder\n",
    "# !zip -r dataloaders.zip ./dataloaders\n",
    "\n",
    "# save the zipped folder in my Gdrive\n",
    "# !cp dataloaders.zip '/content/drive/MyDrive/ETHZ/AI4Good/'\n",
    "# !ls -lt '/content/drive/MyDrive/ETHZ/AI4Good/' \n",
    "\n",
    "# from google.colab import files\n",
    "# files.download('./dataloaders.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNzuG7qGMtS3"
   },
   "outputs": [],
   "source": [
    "del train_loader\n",
    "del val_loader\n",
    "del test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-BOOFJ9MtyQ"
   },
   "outputs": [],
   "source": [
    "# Once dataloaders are available in the working directory, I can just run the cells from here until the end to run a full experiment\n",
    "# Load the dataloader\n",
    "train_loader = torch.load('./dataloaders/train_dataloader.pt')\n",
    "val_loader = torch.load('./dataloaders/val_dataloader.pt')\n",
    "test_loader = torch.load('./dataloaders/test_dataloader.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WandB settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "KFQEQj3xM9JP",
    "outputId": "e2afdee2-57ff-4223-93f5-0ebbed2eb19d"
   },
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"AI4Good-ReforesTree-Main\"\n",
    "\n",
    "# wandb.init()\n",
    "\n",
    "# wandb.init(project=\"AI4Good-ReforesTree\", \n",
    "#            entity=\"Klim\",\n",
    "#            config={\n",
    "#                \"epochs\": N_EPOCHS,\n",
    "#                \"batch_size\": BATCH_SIZE,\n",
    "#                \"learning_rate\": LR,\n",
    "#                \"optimizer\": OPTIMIZER,\n",
    "#                \"criterion\": CRITERION,\n",
    "#                \"device\": DEVICE,\n",
    "#                \"dataset\": \"reforestree\"\n",
    "#            })\n",
    "\n",
    "wandb.init(config={\n",
    "               \"epochs\": N_EPOCHS,\n",
    "               \"batch_size\": BATCH_SIZE,\n",
    "               \"learning_rate\": LR,\n",
    "               \"optimizer\": OPTIMIZER,\n",
    "               \"criterion\": CRITERION,\n",
    "               \"device\": DEVICE,\n",
    "               \"dataset\": \"reforestree\"\n",
    "           })\n",
    "\n",
    "# set model to wandb\n",
    "wandb.watch(model, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqnmBZKNMt0u"
   },
   "outputs": [],
   "source": [
    "# If cell outputs wandb.run, we see the live graphs\n",
    "# wandb.run\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 945
    },
    "collapsed": true,
    "id": "X0gsg3kBUCyd",
    "outputId": "aed50082-1cef-470b-cf66-e082a83056f7"
   },
   "outputs": [],
   "source": [
    "# %%wandb\n",
    "\n",
    "# Set the model to GPU if available\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "print(\"Running on GPU: \", torch.cuda.is_available(), \"\\n\")\n",
    "# Run actual solution and returned the trained model within the trainer \n",
    "model_trainer = run_solution(model, train_loader, val_loader, CRITERION, OPTIMIZER, BATCH_SIZE, N_EPOCHS, LR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Display data images with values in WandB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSFvPdNRyzZx"
   },
   "outputs": [],
   "source": [
    "## Display a few samples of the dataloaders in wandb\n",
    "nb_of_batches_to_display_from = 3\n",
    "nb_of_samples_per_batch_to_display = 2\n",
    "data = []\n",
    "\n",
    "iter_train_loader = iter(train_loader)\n",
    "iter_val_loader = iter(val_loader)\n",
    "iter_test_loader = iter(test_loader)\n",
    "\n",
    "for i in range(nb_of_batches_to_display_from):   # iterate over batches\n",
    "\n",
    "    if i % 3 == 0:\n",
    "        tree_image_batch, tree_agb_value_batch, tree_group_label_batch = next(iter_train_loader) \n",
    "    elif i % 3 == 1:\n",
    "        tree_image_batch, tree_agb_value_batch, tree_group_label_batch = next(iter_val_loader) \n",
    "    elif i % 3 == 2:\n",
    "        tree_image_batch, tree_agb_value_batch, tree_group_label_batch = next(iter_test_loader) \n",
    "\n",
    "    for j in range(nb_of_samples_per_batch_to_display):   # iterate over samples in batch i\n",
    "\n",
    "        tree_image = tree_image_batch[j]\n",
    "        tree_agb_value = tree_agb_value_batch[j]\n",
    "        tree_group_label = tree_group_label_batch[j]\n",
    "\n",
    "        # print(tree_image.shape) \n",
    "        # print(tree_agb_value.shape) \n",
    "        # print(tree_group_label.shape) \n",
    "\n",
    "        image = wandb.Image(tree_image, caption=f\"Tree crown image in batch {i} sample {j}\")  # convert Tensor image to wandb image format\n",
    "        \n",
    "        tree_group_label = torch.argmax(tree_group_label)   # since the tree group labels are OHE, get the label as 0=non-banana and 1=banana\n",
    "\n",
    "        data.append([image, tree_group_label, tree_agb_value])\n",
    "\n",
    "images_table = wandb.Table(columns=[\"image\", \"group label\", \"target_gt\"], data=data)\n",
    "\n",
    "# images_table.add_column(\"id\", ids)\n",
    "# images_table.add_column(\"image\", images)\n",
    "# images_table.add_column(\"group label\", group_labels)\n",
    "# images_table.add_column(\"target_prediction\", agb_values)\n",
    "\n",
    "# Log the Table to W&B\n",
    "wandb.log({\"some ReforesTree data in the loaders\": images_table})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQrVTYpow3ip"
   },
   "source": [
    "### **Perform inference on the test set to evaluate the final performance of the trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "kPow5Olvw2pQ",
    "outputId": "170364cc-7bc6-495d-d042-6524d32fd01a"
   },
   "outputs": [],
   "source": [
    "test_output, test_targets = model_trainer.predict(test_loader)\n",
    "\n",
    "test_output = test_output.flatten()   # to have exactly same size as targets\n",
    "# targets = np.expand_dims(targets, 1)    # to have exactly same size as output\n",
    "\n",
    "print(\"\\Prediction with biggest difference to ground truth: \", np.max(np.abs(test_output-test_targets)))\n",
    "print(\"\\n\\nOnly print the predictions and targets:\\n\")\n",
    "print(\"Predictions: \\n\", test_output)\n",
    "print(\"Targets: \\n\", test_targets)\n",
    "print(\"Difference between output and targets: \\n\", test_output-test_targets)\n",
    "print(\"Output shape: \", test_output.shape)\n",
    "print(\"Targets shape: \", test_targets.shape)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "# error_from_scratch = np.sqrt(((output-targets)**2).mean())  # compute RMSE in pure numpy\n",
    "# print(error_from_scratch)  \n",
    "test_error = mean_squared_error(test_targets, test_output, squared=False)      # with squared=False we get the RMSE (as in the paper)\n",
    "\n",
    "print(\"--- TESTING EVALUATION ---\")\n",
    "print(f'Root Mean Squared Error (on test set): {test_error:.5f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Display TEST data images with values in WandB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9HTtM5Zw2rU"
   },
   "outputs": [],
   "source": [
    "## Display a few samples of the testloader as well as predictions in wandb\n",
    "nb_of_batches_to_display_from = 3\n",
    "nb_of_samples_per_batch_to_display = 2\n",
    "data = []\n",
    "\n",
    "iter_test_loader = iter(test_loader)\n",
    "for i in range(nb_of_batches_to_display_from):  # iterate over batches\n",
    "    \n",
    "    tree_image_batch, tree_agb_value_batch, tree_group_label_batch = next(iter_test_loader) \n",
    "\n",
    "    for j in range(nb_of_samples_per_batch_to_display):   # iterate over samples in batch i\n",
    "\n",
    "\n",
    "        tree_image = tree_image_batch[j]\n",
    "        tree_agb_value = tree_agb_value_batch[j]\n",
    "        tree_group_label = tree_group_label_batch[j]\n",
    "\n",
    "        # print(tree_image.shape) \n",
    "        # print(tree_agb_value.shape) \n",
    "        # print(tree_group_label.shape) \n",
    "\n",
    "        image = wandb.Image(tree_image, caption=f\"Tree crown image in batch {i} sample {j}\")      # convert Tensor image to wandb image format\n",
    "        \n",
    "        tree_group_label = torch.argmax(tree_group_label)   # since the tree group labels are OHE, get the label as 0=non-banana and 1=banana\n",
    "\n",
    "        data.append([image, tree_group_label, tree_agb_value, test_output[i*(BATCH_SIZE - 1) + j]])   # Correct how I get the predictions related to the gt agb value??? We get the first element of each batch i\n",
    "\n",
    "images_table = wandb.Table(columns=[\"image\", \"group label\", \"target_gt\", \"target_prediction\"], data=data)\n",
    "\n",
    "# Log the Table to W&B\n",
    "wandb.log({\"some ReforesTree data in the testloader + predictions\": images_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAtjdPgAxDBE"
   },
   "outputs": [],
   "source": [
    "# experiment_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_trainer.model.state_dict(), '../ckpt/trained_model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eICQVfahRzyy",
    "udG6LpFej51J",
    "QBzltiJ8pRnd",
    "MIeGjVP5oPm7",
    "qzjZZrxSoCCx"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0432cf1a03b3935d80c6a11936d888bc38de41768fd4244f7b0fed64ccdaa5b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
